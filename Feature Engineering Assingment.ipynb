{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2124061",
   "metadata": {},
   "source": [
    "# Question 1st:-What is a parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6e9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Alice!\n"
     ]
    }
   ],
   "source": [
    "# A parameter is a variable used in a function or method definition to accept input values. \n",
    "# It acts as a placeholder for the actual values (called arguments) that are passed to the function when it is called.\n",
    "\n",
    "# Example:-\n",
    "\n",
    "def greet(name):  # \"name\" is the parameter\n",
    "    print(f\"Hello, {name}!\")\n",
    "    \n",
    "greet(\"Alice\")  # \"Alice\" is the argument\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7577c0e",
   "metadata": {},
   "source": [
    "# Question 2nd:-What is correlation?\n",
    "# What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7948443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Correlation?\n",
    "\n",
    "   # Correlation is a statistical measure that describes the strength and direction of the relationship between two variables. It tells us how one variable changes in relation to another.\n",
    "\n",
    "   # Positive Correlation: When one variable increases, the other also increases.\n",
    "   # Negative Correlation: When one variable increases, the other decreases.\n",
    "   # No Correlation: There is no consistent relationship between the variables.\n",
    "   # The strength and direction of correlation are measured using the correlation coefficient (r), which ranges from -1 to +1:\n",
    "\n",
    "  # +1: Perfect positive correlation.\n",
    "  # -1: Perfect negative correlation.\n",
    "  # 0: No correlation.\n",
    "\n",
    "\n",
    "\n",
    "# What Does Negative Correlation Mean?\n",
    "\n",
    "\n",
    "     # A negative correlation means that as one variable increases, the other variable decreases, and vice versa.\n",
    "\n",
    "       # For example:\n",
    "\n",
    "             # Variable A (study hours) and Variable B (time spent playing games) might have a negative correlation. The more time spent studying, the less time is spent playing games.\n",
    "             # Graphically, a negative correlation often shows a downward sloping trend in a scatterplot.\n",
    "\n",
    "    # Example:\n",
    "        # If the correlation coefficient ùëü = ‚àí 0.8, it indicates:\n",
    "\n",
    "    # A strong negative relationship (changes in one variable significantly affect the other in the opposite direction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb2165",
   "metadata": {},
   "source": [
    "# Question 3rd:-Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f6f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Machine Learning\n",
    "\n",
    "         # Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems that can automatically learn \n",
    "         # and improve from experience without being explicitly programmed. It uses data and algorithms to identify patterns, make decisions, or make predictions.\n",
    "\n",
    "# components in Machine Learning:-\n",
    "\n",
    "     # 1.Data: The foundation for learning.\n",
    "     # 2.Features: Attributes used for training.\n",
    "     # 3.Model: The representation of learned patterns.\n",
    "     # 4.Algorithm: The process of optimization.\n",
    "     # 5.Training: Learning from data.\n",
    "     # 6.Evaluation: Measuring performance.\n",
    "     # 7.Prediction: Making decisions or outputs.\n",
    "     # 8.Feedback: Continuous improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5738873a",
   "metadata": {},
   "source": [
    "# Question 4th:-How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415316b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Does Loss Value Help in Determining Model Quality?\n",
    "     # The loss value measures how far the model's predictions are from the actual target values. It is a key indicator of the model's performance during training and evaluation.\n",
    "\n",
    "           # Low Loss: Indicates the model is making accurate predictions.\n",
    "           # High Loss: Indicates the model is making inaccurate predictions.\n",
    "            \n",
    "     # The goal in training a machine learning model is to minimize the loss value by adjusting the model‚Äôs parameters.\n",
    "\n",
    "# Types of Loss Functions\n",
    "     # The choice of the loss function depends on the task:\n",
    "\n",
    "     # Regression Problems: Use loss functions like Mean Squared Error (MSE) or Mean Absolute Error (MAE).\n",
    "     # Classification Problems: Use loss functions like Cross-Entropy Loss or Hinge Loss.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b1e2f",
   "metadata": {},
   "source": [
    "# Question 5th:- What are continuous and categorical variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd9b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Continuous Variables:-\n",
    "\n",
    "     # A continuous variable represents quantitative data that can take any value within a given range. These variables are often measurements and can have decimal or fractional values.\n",
    "\n",
    "# Key Characteristics:\n",
    "     # Can take an infinite number of values within a range.\n",
    "     # Typically numeric (e.g., height, weight, temperature).\n",
    "# Examples:\n",
    "\n",
    "     # Height: 160.5 cm, 172.3 cm.\n",
    "     # Weight: 55.2 kg, 78.4 kg.\n",
    "     # Temperature: 21.5¬∞C, 37.2¬∞C.\n",
    "     # Time: 1.5 hours, 3.75 seconds.\n",
    "\n",
    "        \n",
    "# 2. Categorical Variables:-\n",
    "\n",
    "    # A categorical variable represents data that can be divided into distinct groups or categories. These variables are typically qualitative and may or may not have a logical order.\n",
    "\n",
    "# Key Characteristics:\n",
    "\n",
    "    # Values are labels or categories, not numbers.\n",
    "    # Can be:\n",
    "           # Nominal: No inherent order (e.g., gender, colors).\n",
    "           # Ordinal: Ordered categories (e.g., satisfaction levels: low, medium, high).\n",
    "# Examples:\n",
    "    # Gender: Male, Female, Other.\n",
    "    # Color: Red, Blue, Green.\n",
    "    # Education Level: High School, Bachelor‚Äôs, Master‚Äôs.\n",
    "    # Customer Satisfaction: Satisfied, Neutral, Unsatisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84f52a",
   "metadata": {},
   "source": [
    "# Question 6th:-How do we handle categorical variables in Machine Learning? What are the common t echniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7df1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Categorical Variables in Machine Learning:-\n",
    "\n",
    "    # Categorical variables need to be converted into numbers. Here are common techniques:\n",
    "\n",
    "        # 1.Label Encoding: Convert each category into a unique integer.\n",
    "\n",
    "             # Example: Red ‚Üí 0, Blue ‚Üí 1, Green ‚Üí 2.\n",
    "        # 2.One-Hot Encoding: Create binary columns (1 or 0) for each category.\n",
    "\n",
    "             # Example: Red ‚Üí [1, 0, 0], Blue ‚Üí [0, 1, 0], Green ‚Üí [0, 0, 1].\n",
    "        # 3.Ordinal Encoding: Assign integers based on the order of categories.\n",
    "\n",
    "             # Example: Small ‚Üí 0, Medium ‚Üí 1, Large ‚Üí 2.\n",
    "        # 4.Frequency Encoding: Replace categories with their frequency/count in the dataset.\n",
    "\n",
    "             # Example: Red (100 occurrences), Blue (50), Green (25).\n",
    "        # 5.Target Encoding: Replace categories with the average value of the target variable for that category.\n",
    "\n",
    "             # Example: Red ‚Üí 0.8, Blue ‚Üí 0.6.\n",
    "        # 6.Binary Encoding: Convert categories into binary code to reduce dimensionality.\n",
    "\n",
    "             # Example: A ‚Üí 001, B ‚Üí 010, C ‚Üí 011.\n",
    "        # 7.Embedding (for Deep Learning): Use learned dense vector representations for categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f475b",
   "metadata": {},
   "source": [
    "# Question 7th:-What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1089f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing a dataset are key steps in machine learning:-\n",
    "\n",
    "     # Training: This involves using a portion of the dataset to teach the model. The model learns patterns \n",
    "        # and relationships in the data to make predictions or classifications.\n",
    "\n",
    "     # Testing: After training, the model is evaluated on a separate portion of \n",
    "        # the dataset (called the test set). This helps assess how well the model performs on unseen data and whether it can generalize its learning beyond the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de2b8e",
   "metadata": {},
   "source": [
    "# Question 8th:-What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e7ecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn.preprocessing:-\n",
    "\n",
    "    # sklearn.preprocessing is a module in Scikit-learn (a popular Python library for machine learning) that provides tools to preprocess and transform data before feeding it into a machine learning model. Preprocessing is important because raw data often needs to be standardized, normalized, or encoded to improve the model's performance.\n",
    "\n",
    "# Key Features of sklearn.preprocessing:\n",
    "    # 1.Scaling and Normalization:\n",
    "\n",
    "        # StandardScaler: Scales features to have a mean of 0 and a standard deviation of 1.\n",
    "        # MinMaxScaler: Scales data to a specific range (e.g., [0, 1]).\n",
    "        # Normalizer: Normalizes each sample to have unit norm (used for sparse data).\n",
    "    # 2.Encoding Categorical Data:\n",
    "\n",
    "        # LabelEncoder: Converts labels into numeric form.\n",
    "        # OneHotEncoder: Encodes categorical features as binary arrays.\n",
    "    # 3.Imputation of Missing Values:\n",
    "\n",
    "        # SimpleImputer: Replaces missing values with a specific value (e.g., mean, median).\n",
    "    # 4.Binarization:\n",
    "\n",
    "        # Binarizer: Converts numerical data into binary values based on a threshold.\n",
    "    # 5.Polynomial Features:\n",
    "\n",
    "        # PolynomialFeatures: Generates new features based on polynomial combinations of existing ones.\n",
    "   # 6.Custom Transformation:\n",
    "\n",
    "        # FunctionTransformer: Apply custom transformations to your data.\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "data = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf86bd1",
   "metadata": {},
   "source": [
    "# Question 9th:-What is a Test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4bcd107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: [[5], [3], [1], [4]] [5, 3, 1, 4]\n",
      "Test set: [[2]] [2]\n"
     ]
    }
   ],
   "source": [
    "# Test set:- A test set is a portion of a dataset used to evaluate the performance of a trained machine learning model. \n",
    "            # It consists of data that the model has not seen during the training process. The purpose of the test set is to check how well the model generalizes to new, unseen data.\n",
    "\n",
    "# Key Points:\n",
    "    # Purpose: To measure the model's accuracy, precision, recall, or other performance metrics.\n",
    "    # Separation: Typically, the dataset is split into a training set and a test set (e.g., 80% for training and 20% for testing).\n",
    "    # Real-world simulation: The test set mimics how the model will perform on future, real-world data.\n",
    "\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample dataset\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train set:\", X_train, y_train)\n",
    "print(\"Test set:\", X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dc5b3",
   "metadata": {},
   "source": [
    "# Question 10th:-How do we split data for model fitting (training and testing) in Python?\n",
    "# How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35d3de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Split Data for Model Fitting in Python:-\n",
    "    # Use train_test_split from Scikit-learn:-\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# X: Features (input data).\n",
    "# y: Labels (target values).\n",
    "# test_size=0.2: 20% of the data for testing.\n",
    "# random_state: Ensures consistent results.\n",
    "\n",
    "# Approach to a Machine Learning Problem:-\n",
    "\n",
    "    # 1.Understand the Problem: Define the objective.\n",
    "    # 2.Collect Data: Gather and inspect the dataset.\n",
    "    # 3.Preprocess Data: Clean and prepare the data (e.g., handle missing values).\n",
    "    # 4.Split Data: Divide into training and testing sets.\n",
    "    # 5.Feature Engineering: Select or create relevant features.\n",
    "    # 6.Model Selection: Choose a suitable algorithm and train the model.\n",
    "    # 7.Evaluation: Test the model using the test set and measure performance.\n",
    "    # 8.Optimization: Fine-tune the model (e.g., hyperparameter tuning).\n",
    "    # 9.Deployment: Implement the model in a real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b81158",
   "metadata": {},
   "source": [
    "# Question 11th:-Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f012cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We perform Exploratory Data Analysis (EDA) before fitting a model to:-\n",
    "\n",
    "     # 1.Understand the Data: Identify patterns, distributions, and relationships.\n",
    "     # 2.Detect Issues: Spot missing values, outliers, or inconsistencies.\n",
    "     # 3.Feature Insights: Select or engineer relevant features.\n",
    "     # 4.Guide Preprocessing: Decide on scaling, encoding, or transformations.\n",
    "     # 5.Avoid Bias: Ensure the data is balanced and representative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbdadbc",
   "metadata": {},
   "source": [
    "# Question 12th:- What is correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "114f8786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Correlation:-\n",
    "    # Correlation is a statistical measure that indicates the strength and direction of a relationship between two variables.\n",
    "\n",
    "# Key Points:-\n",
    "    # 1.Range: Correlation values range from -1 to 1:\n",
    "\n",
    "          # +1: Strong positive relationship (as one increases, the other increases).\n",
    "          # -1: Strong negative relationship (as one increases, the other decreases).\n",
    "          # 0: No relationship.\n",
    "    # Types:\n",
    "\n",
    "          # Pearson Correlation: Measures linear relationships.\n",
    "          # Spearman Correlation: Measures rank-based relationships.\n",
    "    # Usage:\n",
    "\n",
    "          # To identify patterns between variables.\n",
    "          # To select important features for a model.\n",
    "            \n",
    "            \n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Compute correlation\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(\"Correlation:\", correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f2868",
   "metadata": {},
   "source": [
    "# Question 13th:-What does negative correlation mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e9c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A negative correlation means that as one variable increases, the other decreases.\n",
    "\n",
    "# Example:\n",
    "\n",
    "   # Higher temperatures ‚Üí Lower sales of winter coats.\n",
    "   # Correlation value: Between -1 (strong negative) and 0 (no correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9a530",
   "metadata": {},
   "source": [
    "# Question 14th:How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e61111ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "A  1.0 -1.0\n",
      "B -1.0  1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "# Using Pandas:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'A': [1, 2, 3], 'B': [3, 2, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute correlation\n",
    "correlation = df.corr()\n",
    "print(correlation)\n",
    "\n",
    "\n",
    "# using Numpy :-\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "x = [1, 2, 3]\n",
    "y = [3, 2, 1]\n",
    "\n",
    "# Compute correlation\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(correlation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01176f53",
   "metadata": {},
   "source": [
    "# Question 15th:What is causation? Explain difference between correlation and causation with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232817f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causation:\n",
    "\n",
    "      # Causation means that one variable directly affects or causes a change in another.\n",
    "    \n",
    "# Difference Between Correlation and Causation:-\n",
    "\n",
    "      # Correlation: Shows a relationship between two variables but doesn't prove one causes the other.\n",
    "    \n",
    "      # Causation: Implies that changes in one variable directly lead to changes in the other.\n",
    "\n",
    "# Example:\n",
    "\n",
    "     # Correlation: Ice cream sales and drowning incidents are positively correlated, but eating ice cream doesn't cause drowning.\n",
    "    \n",
    "     # Causation: Higher temperatures (cause) lead to both increased ice cream sales and more swimming (effect), which increases drowning risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b0420",
   "metadata": {},
   "source": [
    "# Question 16th:-What is an Optimizer? What are different types of optimizers? Explain each with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b84f1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer:-\n",
    "\n",
    "     # An optimizer is an algorithm used in machine learning and deep learning to minimize or maximize an objective function, often the loss function, to improve the model's performance.\n",
    "\n",
    "# Types of Optimizers:\n",
    "  # 1.Gradient Descent:\n",
    "\n",
    "      # Description: Updates model parameters by moving in the direction of the steepest decrease in the loss function.\n",
    "      # Example: In a simple linear regression, gradient descent adjusts the weights to reduce the error between predicted and actual values.\n",
    "  # 2.Stochastic Gradient Descent (SGD):\n",
    "\n",
    "      # Description: A variant of gradient descent that updates weights using a single data point (or mini-batch) at a time, leading to faster convergence.\n",
    "      # Example: In training a neural network, SGD updates the weights after evaluating one training example.\n",
    "  # 3.Mini-Batch Gradient Descent:\n",
    "\n",
    "      # Description: Combines the benefits of both batch gradient descent and SGD by updating weights using a small batch of data.\n",
    "      # Example: Commonly used in deep learning when training on large datasets.\n",
    "  # 4.Momentum:\n",
    "\n",
    "      # Description: Adds a \"velocity\" term to the update process, helping the model avoid getting stuck in local minima by smoothing out updates.\n",
    "      # Example: In training, momentum helps the model converge faster by considering past gradients.\n",
    "  # 5.Adam (Adaptive Moment Estimation):\n",
    "\n",
    "      # Description: An adaptive optimizer that computes adaptive learning rates for each parameter by combining the advantages of momentum and RMSProp.\n",
    "      # Example: Commonly used in training deep learning models because it performs well on a wide range of problems without much tuning.\n",
    "  # 6.RMSProp (Root Mean Square Propagation):\n",
    "\n",
    "      # Description: An adaptive optimizer that adjusts the learning rate for each parameter based on its recent gradient magnitudes.\n",
    "      # Example: Often used in recurrent neural networks (RNNs) and deep learning tasks for better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f5acf5",
   "metadata": {},
   "source": [
    "# Question 17th:-What is sklearn.linear_model ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e67fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.linear_model is a module in Scikit-learn that provides tools for linear models used in machine learning. These models predict a target variable based on a linear relationship with the input features.\n",
    "\n",
    "# Key Models:\n",
    "    # LinearRegression: For predicting continuous values.\n",
    "    # LogisticRegression: For binary classification.\n",
    "    # Ridge: Linear regression with L2 regularization.\n",
    "    # Lasso: Linear regression with L1 regularization (feature selection).\n",
    "    # ElasticNet: Combines L1 and L2 regularization.\n",
    "    # SGDClassifier/SGDRegressor: Linear models using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3ddff",
   "metadata": {},
   "source": [
    "# Question 18th:-What does model.fit() do? What arguments must be given?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60ff6b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit() trains the model using the input data and target labels.\n",
    "\n",
    "# Arguments:\n",
    "    # X: Input features (e.g., training data).\n",
    "    # y: Target labels (e.g., outcomes or values to predict).\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X_train = [[1], [2], [3]]  # Input features\n",
    "y_train = [2, 4, 6]        # Target values\n",
    "\n",
    "# Create a model and fit it to the data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # Fit the model to training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bd443",
   "metadata": {},
   "source": [
    "# Question 19th:-What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f8a9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict() makes predictions based on the trained model and new input data.\n",
    "\n",
    "# Arguments:\n",
    "\n",
    "    # X: The input data (features) for which predictions are needed. It should match the shape and format of the training data.\n",
    "    \n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# Assume the model is already trained with model.fit()\n",
    "\n",
    "X_new = [[4], [5]]  # New input data\n",
    "predictions = model.predict(X_new)  # Predict target values for X_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e6353",
   "metadata": {},
   "source": [
    "# Question 20th:-What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "905e1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Continuous Variables:-\n",
    "\n",
    "     # A continuous variable represents quantitative data that can take any value within a given range. These variables are often measurements and can have decimal or fractional values.\n",
    "\n",
    "# Key Characteristics:\n",
    "     # Can take an infinite number of values within a range.\n",
    "     # Typically numeric (e.g., height, weight, temperature).\n",
    "# Examples:\n",
    "\n",
    "     # Height: 160.5 cm, 172.3 cm.\n",
    "     # Weight: 55.2 kg, 78.4 kg.\n",
    "     # Temperature: 21.5¬∞C, 37.2¬∞C.\n",
    "     # Time: 1.5 hours, 3.75 seconds.\n",
    "\n",
    "        \n",
    "# 2. Categorical Variables:-\n",
    "\n",
    "    # A categorical variable represents data that can be divided into distinct groups or categories. These variables are typically qualitative and may or may not have a logical order.\n",
    "\n",
    "# Key Characteristics:\n",
    "\n",
    "    # Values are labels or categories, not numbers.\n",
    "    # Can be:\n",
    "           # Nominal: No inherent order (e.g., gender, colors).\n",
    "           # Ordinal: Ordered categories (e.g., satisfaction levels: low, medium, high).\n",
    "# Examples:\n",
    "    # Gender: Male, Female, Other.\n",
    "    # Color: Red, Blue, Green.\n",
    "    # Education Level: High School, Bachelor‚Äôs, Master‚Äôs.\n",
    "    # Customer Satisfaction: Satisfied, Neutral, Unsatisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c40c1",
   "metadata": {},
   "source": [
    "# Question 21th:-What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7bd369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling is the process of standardizing or normalizing the range of features (input data) so that they have similar scales.\n",
    "\n",
    "# Why It's Important:\n",
    "\n",
    "    # Improves Model Performance: Many algorithms (e.g., gradient descent, k-nearest neighbors) perform better when features are on similar scales.\n",
    "    # Prevents Bias: Features with larger ranges can dominate the model's learning process.\n",
    "# Types:\n",
    "\n",
    "    # Standardization: Centers data to have a mean of 0 and a standard deviation of 1.\n",
    "    # Normalization: Scales data to a fixed range, typically [0, 1]\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3bb656",
   "metadata": {},
   "source": [
    "# Question 22th:-How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1661913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can perform feature scaling in Python using Scikit-learn's StandardScaler or MinMaxScaler.\n",
    "\n",
    "# 1. Standardization (Z-score scaling):\n",
    "      # Scales features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your data\n",
    "\n",
    "\n",
    "# 2. Normalization (Min-Max scaling):\n",
    "     # Scales features to a specific range, typically [0, 1].\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634488b2",
   "metadata": {},
   "source": [
    "# Question 23th:-What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2263c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.preprocessing is a module in Scikit-learn that provides tools for data preprocessing. It helps prepare data for machine learning models by transforming or scaling features.\n",
    "\n",
    "\n",
    "# Key Functions:\n",
    "\n",
    "    # StandardScaler: Standardizes features (mean=0, std=1).\n",
    "    # MinMaxScaler: Scales features to a specific range, usually [0, 1].\n",
    "    # LabelEncoder: Encodes categorical labels as numeric values.\n",
    "    # OneHotEncoder: Converts categorical features into binary vectors.\n",
    "    # PolynomialFeatures: Generates polynomial and interaction features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29f858",
   "metadata": {},
   "source": [
    "# Question 24th:-How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23f33734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can split data for model fitting using train_test_split from Scikit-\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5]]  # Features\n",
    "y = [1, 2, 3, 4, 5]            # Target labels\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691840f8",
   "metadata": {},
   "source": [
    "# Question 25th:-Explain data encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0df4f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data encoding is the process of converting categorical data (e.g., text labels) into a numerical format that can be used by machine learning models.\n",
    "\n",
    "# 1.Label Encoding:\n",
    "\n",
    "    # Converts each category into a unique integer.\n",
    "    \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = ['cat', 'dog', 'dog', 'cat']\n",
    "y_encoded = le.fit_transform(y)  # Output: [0, 1, 1, 0]\n",
    "\n",
    "# 2.One-Hot Encoding:\n",
    "\n",
    "     # Converts each category into a binary vector, with 1 in the column corresponding to the category and 0 in all others.\n",
    "    \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "X = [['cat'], ['dog'], ['dog'], ['cat']]\n",
    "X_encoded = encoder.fit_transform(X)  # Output: [[1, 0], [0, 1], [0, 1], [1, 0]]\n",
    "\n",
    "\n",
    "# 3.Ordinal Encoding:\n",
    "\n",
    "     # Encodes categories with a meaningful order (e.g., low, medium, high) into numerical values.\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X = [['low'], ['high'], ['medium'], ['low']]\n",
    "X_encoded = encoder.fit_transform(X)  # Output: [[0], [2], [1], [0]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c111c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
